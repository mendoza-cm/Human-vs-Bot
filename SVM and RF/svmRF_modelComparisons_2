rm(list=ls())
library(car)
library(e1071)
library(tidyverse)
library(pROC)
library(webshot2)
load("~/myGit_R/Data/bot_vs_human/human_vs_bot_dataUpdated.RData") # will need training_data
load("~/myGit_R/Data/svmRF_models_comparison.RData") ## new models saved insvmRF_modelComparisons_1.R
load("~/myGit_R/Data/testData_ml_class_1.RData") # test_data with saved scores form first round of ML predictions




#### classify test data
testData_ML <- test_data_ML1 %>%
				dplyr::mutate(svm_2Linear_pred = predict(myModelsRound2$svm$linear,.),
				 svm_2RFB_pred = predict(myModelsRound2$svm$rfb,.),
				 svm_2Poly2_pred = predict(myModelsRound2$svm$poly2,.),
				 svm_2Poly3_pred = predict(myModelsRound2$svm$poly3,.),
				 rf_2_pred = predict(myModelsRound2$RF,.)
				) %>%
		dplyr::mutate_at(.vars = vars(svm_2Linear_pred,
							svm_2RFB_pred,
							svm_2Poly2_pred,
							svm_2Poly3_pred,
							rf_2_pred), ~
							as.logical(.)) %>%
		dplyr::mutate(svm_2Linear_predProb = attr(predict(myModelsRound2$svm$linear,., probability = TRUE),"probabilities")[,"TRUE"],
				 svm_2RFB_predProb = attr(predict(myModelsRound2$svm$rfb,., probability = TRUE),"probabilities")[,"TRUE"],
				 svm_2Poly2_predProb = attr(predict(myModelsRound2$svm$poly2,., probability = TRUE),"probabilities")[,"TRUE"],
				 svm_2Poly3_predProb = attr(predict(myModelsRound2$svm$poly3,., probability = TRUE),"probabilities")[,"TRUE"],
				 rf_2_predProb = predict(myModelsRound2$RF,., type = "prob")[,"TRUE"]
				) 




my_ML_predNames_svm2 <- setdiff(grep("_2",names(testData_ML), value = TRUE, fixed = TRUE),
						grep("_predProb",names(testData_ML), value = TRUE, fixed = TRUE))
myClassTesting2 <- data.frame(
	
	classifier = my_ML_predNames_svm2			

)
for(predName in my_ML_predNames_svm2){


	myClassTesting2[which(myClassTesting2[,"classifier"] == predName),"true_positives"] <- length(which(testData_ML[,predName] == "TRUE" & testData_ML$isBot))
	myClassTesting2[which(myClassTesting2[,"classifier"] == predName),"true_negatives"] <- length(which(testData_ML[,predName] == "FALSE" & !testData_ML$isBot))
		
	myClassTesting2[which(myClassTesting2[,"classifier"] == predName),"Accuracy"] <- round(
										(myClassTesting2[which(myClassTesting2[,"classifier"] == predName),2] + 
												myClassTesting2[which(myClassTesting2[,"classifier"] == predName),3])/240
															,3)

	roc_obj <- roc(testData_ML$isBot,testData_ML[,paste0(predName,"Prob")])
	myClassTesting2[which(myClassTesting2[,"classifier"] == predName),"Model AUC"] <- round(auc(roc_obj),3)



	}

oob_accuracy2 <- 1 - myModelsRound2$RF$err.rate[nrow(myModelsRound2$RF$err.rate), "OOB"]
oob_pred2 <- predict(myModelsRound2$RF)  # by default uses OOB votes if no newdata is given
oob_accuracy2 <- mean(oob_pred2 == training_data$isBot)


oob_probs_2Preds <- predict(myModelsRound2$RF, type = "prob")[, "TRUE"]
roc_oob2 <- roc(response = training_data$isBot, predictor = oob_probs_2Preds)
auc_oob2 <- auc(roc_oob2)
RF_OOB_TP2 <- myModelsRound2$RF$confusion[[1]]
RF_OOB_TN2 <- myModelsRound2$RF$confusion[[4]]


myClassTesting2_OOB <- rbind.fill(myClassTesting2,data.frame(classifier = "RF_OOB",
									true_positives = RF_OOB_TP2,
									true_negatives = RF_OOB_TN2,
									Accuracy = round(oob_accuracy2,3)))

myClassTesting2_OOB[nrow(myClassTesting2_OOB),ncol(myClassTesting2_OOB)] <- round(auc_oob2,3)


### switch the RF models: place with othr models using same number of vars for training
### usually would never check RF with less vars - throw everything into ensemble methods

rf_allPred <- myClassTesting_OOB[5:6,]
myClassTesting_OOB[5:6,] <- myClassTesting2_OOB[5:6,]
myClassTesting2_OOB[5:6,] <- rf_allPred

#rfPRedictions <-predict(rf_model2,test_data, type = "prob")


myClassifierTable <- cbind(data.frame(ClassifierType = 
					c("SVM: Linear Model","SVM: Radial Model","SVM: Polynomial Degree 2",
						"SVM: Polynomial Degree 3","RF Model","RF OOB")),
					cbind(myClassTesting_OOB[,2:5],myClassTesting2_OOB[,2:5]))

library(kableExtra)
tbl_html <- kable(
  myClassifierTable,
  col.names = c(
    "Classifier",
    "TP", "TN", "Accuracy", "Model AUC",   # for 2-variable models
    "TP", "TN", "Accuracy", "Model AUC"   # for all-predictor models
  ),
  caption = "Comparison of Classifier Performance"
) %>%
  add_header_above(c(
    " " = 1,
    "2-Variable Model" = 4,
    "All Predictors from Data"   = 4
  )) %>%
  row_spec(c(2,4,5), background = "rgba(144,238,144,0.3)") %>%
  column_spec(c(4,5,8,9), background = "rgba(195,98,148,0.3)") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE
  )
#save_kable(tbl_html, "~/myGit_R/Visualizations/classifier_comparison.png", zoom = 2)
